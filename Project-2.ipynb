{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**Course Project 2** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy**\n",
    "\n",
    "1. You may use your own or any software packages/module/codes to do this project. \n",
    "2. The project has to be done in groups of 2-4 persons each working together to complete all the tasks. The team can be the same as the Project 1 or re-formed.  \n",
    "4. The report should be no more than 20 pages, including one page describing the contributions of each of the members in the group, figures, tables and codes. \n",
    "5. You are encouraged to do beyond the described tasks along with the central topic. Quality and innovative work will be greatly rewarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider a penetration of a bullet onto a plate, as shown in the following figure. \n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"../../../images/Penetration.png\" width=\"300\"/>\n",
    "<br>\n",
    "[A bullet penetrated a circular plate]\n",
    "</div>\n",
    "\n",
    "Study the paper entitle \"Real-time prediction of projectile penetration to laminates by training machine learning models with finite element solver as the trainer\" given in the file holder of the course website. The dataset for this study was generated from a large number of FEM analyses for a real research project, and it is given also in the file folder. The Python code for the neural network is given there as Neural_network_FEA.py. Note that this neural network model is for predicting the entire time history of the velocity of the bullet, before and after the penetration. Before doing your tasks before, you should try to repeat the work on NN first using exactly the same dataset and codes.  \n",
    "\n",
    "1. Establish a new NN model to predict only the residual velocity of the bullet, using the given dataset, aiming to outperform the one in the paper. Note that for the penetrated cases, the residual velocity will still be positive. For the not-penetrated cases, it is negative because the bullet is bounced back. \n",
    "\n",
    "2. Establish any other alternative model to repeat Task 1. \n",
    "\n",
    "3. Discuss on what type of ML model work best for this type of data.\n",
    "\n",
    "You may use any existing 3rd party ML module or your own code to get this tasks done. \n",
    "\n",
    "Please submit both codes and reports online by the deadline. No extension will be given, because this is the final project.Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 2430.9219 - mape: 74.0628\n",
      "RMSE:  49.30437987643694\n"
     ]
    }
   ],
   "source": [
    "# Thesis code Neural Network- Finite Element Dataset\n",
    "# Author- Pushkar Wadagbalkar\n",
    "# ************************************************************************************\n",
    "\n",
    "# importing all the required python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# importing training dataset and target files\n",
    "dataset = pd.read_csv('./Finite_element_entire_data_set.csv')\n",
    "X = dataset.iloc[:, :].values\n",
    "Y = dataset.iloc[:, 5].values\n",
    "X = np.delete(X, 5, 1)\n",
    "\n",
    "# Column Transfer and one hot encoding for categorical features\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[(\"oh\", OneHotEncoder(), [2, 3, 4])], remainder=\"passthrough\")\n",
    "X = ct.fit_transform(X)\n",
    "X = np.asarray(X).astype('float32')\n",
    "# fig, (ob1, ob2) = plt.subplots(ncols=2, figsize=(10, 10))\n",
    "# ob1.set_title('Before Scaling')\n",
    "# sns.kdeplot(dataset['Time'], ax=ob1)\n",
    "# sns.kdeplot(dataset['Velocity'], ax=ob1)\n",
    "# Feature scaling using MinMaxScaler\n",
    "sc = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X[:, [10, 11]] = sc.fit_transform((X[:, [10, 11]]))\n",
    "Z = pd.DataFrame(X)\n",
    "# Plotting the KDE plot for data after feature scaling\n",
    "scaled = Z.iloc[:, :]\n",
    "# ob2.set_title('After Scaling')\n",
    "# sns.kdeplot(scaled[10], ax=ob2)\n",
    "# sns.kdeplot(scaled[11], ax=ob2)\n",
    "# Splitting the training data from the unseen inputs for which predictions are to be made\n",
    "Train = Z.iloc[0:3221]\n",
    "Input = Z.iloc[3221::]\n",
    "# Splitting the data into training, validation and testing datasets\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    Train, Y, test_size=0.20, random_state=415)\n",
    "validation_x, testing_x, validation_y, testing_y = train_test_split(\n",
    "    test_x, test_y, test_size=0.50, random_state=415)\n",
    "# importing and implementing Neural network regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1,))\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['mape'])\n",
    "# Compiling the model with early stopping algorithm\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3,\n",
    "                        patience=10, verbose=0, mode='auto')\n",
    "# Computing training and validation loss for the network\n",
    "history = model.fit(train_x, train_y, validation_data=(\n",
    "    validation_x, validation_y), callbacks=[monitor], verbose=0, epochs=1000)\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "# Getting predictions for training dataset\n",
    "y_train_pred = model.predict(train_x)\n",
    "# Getting predictions for validation dataset\n",
    "y_validation_pred = model.predict(validation_x)\n",
    "# Getting predictions for testing dataset\n",
    "final_result = model.predict(testing_x)\n",
    "# Getting predictions for unseen inputs\n",
    "# Unseen_result = model.predict(Input)\n",
    "# Calculating relative error for validation dataset\n",
    "Error = ((validation_y-y_validation_pred)/validation_y)\n",
    "\n",
    "\n",
    "def average(Error):\n",
    "    return sum(abs(Error))/len(Error)\n",
    "\n",
    "\n",
    "Relative_Error = average(Error)\n",
    "# Calculating relative error for testing dataset\n",
    "Error_testing = ((testing_y-final_result)/testing_y)\n",
    "\n",
    "\n",
    "def average(Error_testing):\n",
    "    return sum(abs(Error_testing))/len(Error_testing)\n",
    "\n",
    "\n",
    "Testing_Relative_Error = average(Error_testing)\n",
    "\n",
    "# Calculate accuracy and error metrics \n",
    "score = model.evaluate(testing_x, testing_y)\n",
    "print('RMSE: ', np.sqrt(score[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Neural Network Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared value:  0.9309973313740452\n",
      "RMSE:  48.488634060681655\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Open file utilizing pandas module for data frames\n",
    "df = pd.read_csv('./Finite_element_entire_data_set.csv', sep=',')\n",
    "\n",
    "# Preform OHE\n",
    "onehot = pd.get_dummies(df, drop_first=True)\n",
    "X = onehot.to_numpy()\n",
    "Y = onehot.iloc[:, 3].values\n",
    "X = np.delete(X, 3, 1)\n",
    "\n",
    "# Split the dataset into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "y_train = y_train.astype('int')\n",
    "\n",
    "# Train Data\n",
    "clf = MLPRegressor(\n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    hidden_layer_sizes=(100, 100),\n",
    "    alpha=0.001,\n",
    "    random_state=20,\n",
    "    early_stopping=False,\n",
    "    max_iter=10000\n",
    ").fit(X_train, y_train)\n",
    "# Predictions and testing/training\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and error metrics\n",
    "test_set_rsquared = clf.score(X_test, y_test)\n",
    "test_set_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print('R_squared value: ', test_set_rsquared)\n",
    "print('RMSE: ', test_set_rmse)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Alt-Model (SVM Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared value:  0.8889380559916655\n",
      "RMSE:  61.516220088465936\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Open file utilizing pandas module for data frames\n",
    "df = pd.read_csv('./Finite_element_entire_data_set.csv', sep=',')\n",
    "\n",
    "# Preform OHE\n",
    "onehot = pd.get_dummies(df, drop_first=True)\n",
    "X = onehot.to_numpy()\n",
    "y = onehot.iloc[:, 3].values\n",
    "X = np.delete(X, 3, 1)\n",
    "\n",
    "\n",
    "# Split the dataset into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "regressor = SVR(kernel='rbf')\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and error metrics\n",
    "test_set_rsquared = regressor.score(X_test, y_test)\n",
    "test_set_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print('R_squared value: ', test_set_rsquared)\n",
    "print('RMSE: ', test_set_rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Type of ML model best for this type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "",
   "title_sidebar": "",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
