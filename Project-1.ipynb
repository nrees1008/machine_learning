{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2b882d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd948b89",
   "metadata": {},
   "source": [
    "# Course Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02242ad",
   "metadata": {},
   "source": [
    "**Policy**\n",
    "\n",
    "1. You may use your own or any software packages/module/codes to do this project. \n",
    "2. The project has to be done in groups of 2-4 persons each working together to complete all the tasks. Only on report for a group needs to be submitted. \n",
    "3. You shall form your own group. The same group may work together again in the 2nd (final) course project. \n",
    "4. The report should be no more than 20 pages, including one page describing the contributions of each of the members in the group, figures, tables and codes. \n",
    "5. You are encouraged to do beyond the described tasks along with the central topic. Quality and innovative work will be greatly rewarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e735ed",
   "metadata": {},
   "source": [
    "**Problem statement**\n",
    "\n",
    "Consider a problem with 2 feature variables, $\\mathbf{x}=[x_1 \\;\\;\\; x_2]$ where $-4.0 \\le x_1 \\le 4.0$, and $-4.0 \\le x_2 \\le 4.0 $. Assume a true label function has the following form known as by-linear function:  \n",
    "\n",
    "\\begin{equation} \\label{eqHW3-1}\n",
    "y(\\mathbf{x})= c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1 x_2\n",
    "\\end{equation} \n",
    "\n",
    "where $c_i (i=0,1,2,3) $ are given constants of real scalar numbers. In the numerical study, use:\n",
    "\n",
    "\\begin{equation} \\label{eqHW3-2}\n",
    "c_0 = 2.1, c_1 = 3.8, c_2 = 1.8, c_3 = 0.5 \n",
    "\\end{equation}  \n",
    "\n",
    "1. Construct a prediction function with proper set of learning parameters, that is capable of predicting the true label function. Define a loss function, and use it to design a possible procedure that is capable of leading to a correct prediction analytically. \n",
    "\n",
    "2. Use a dataset with 4 data points. Define a proper loss function and derive formulas that leads to a unique solution in predicting the true label function analytically. Discuss about the conditions for the uniqueness. \n",
    "\n",
    "3. Create quality dataset each with sufficient number of numerical data points with the numerical label values, using the given true label function. Define a proper loss function and a (Python) code to make predictions. Show that the constants given in Eq.(\\ref{eqHW3-2}) are reproduced from the dataset created by your code. Test your code thoroughly, provide proper conditions on safe-use of your code, and list also possible issues when using your code. \n",
    "\n",
    "4. Create poor dataset that has deficiency in the data matrix. Repeat task 3 and discuss about the possible problems, find a solution to overcome it, and test you solution in a code. \n",
    "\n",
    "5. Create poor dataset that has sufficient data points, but with parallel data points. Repeat task 3 and discuss about the possible problems, find a solution to overcome it, and test you solution in a code.\n",
    "\n",
    "6. Discuss possible issues if the features space dimension is very high, say $p=10,000$. \n",
    "\n",
    "7. Discuss your strategy to construct a neural network to do task 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b6426",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "We can define a prediction function provided we know the feature variables and the weights:\n",
    "\n",
    "$$ z(\\mathbf{\\hat{w}};\\mathbf{x})=[\\mathbf{\\bar{x}}\\mathbf{\\hat{w}}] $$\n",
    "\n",
    "Next, we must provide a model that represents the dataset for training and testing. In this scenario, we are provided a true label function assuming a known by-linear function:\n",
    "\n",
    "$$ y(\\mathbf{x})= c_0 + c_1 x_1 + c_2 x_2 + c_3 x_1 x_2 $$ \n",
    "\n",
    "Where $ c_i (i=0,1,2,3) $ are given constants of real scalar numbers. Next, we define the general form of the true label function and rewrite to solve for the true weights:\n",
    "\n",
    "$$ \\mathbf{\\hat{k}} = \\mathbf{\\bar{x}}y(x)^{-1} $$\n",
    "\n",
    "Now we can define the loss function in terms of our learning parameters. The loss function used here is L2:\n",
    "\n",
    "$$ \\mathcal{L}(z(\\mathbf{\\hat{w}})) = || z(\\mathbf{\\hat{w}};\\mathbf{x}) - y(\\mathbf{\\bar{x}})||^{2} $$ \n",
    "\n",
    "$$ \\mathcal{L}(z(\\mathbf{\\hat{w}})) =  (\\mathbf{\\hat{w}}^\\top - \\mathbf{\\hat{k}}^\\top)[\\mathbf{\\bar{x}}^\\top\\mathbf{\\bar{x}}](\\mathbf{\\hat{w}} - \\mathbf{\\hat{k}}) $$\n",
    "\n",
    "We must now minimize a loss function in order to get a solution. Thus, we define a gradient by taking a partial derivative of the loss function with respect to the weights:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\mathbf{\\hat{w}})}{\\partial \\mathbf{\\hat{w}}}\n",
    "= 2[\\mathbf{\\bar{x}}^\\top\\mathbf{\\bar{x}}](\\mathbf{\\hat{w}} - \\mathbf{\\hat{k}}) = 0 $$\n",
    "\n",
    "We note the goal here is to match the weights to the true weight, thus we attempt to obtain:\n",
    "\n",
    "$$ (\\mathbf{\\hat{w}} - \\mathbf{\\hat{k}}) = 0 $$\n",
    "\n",
    "This will satisfy our minimized loss function, to reproduce our linear function in the feature space; however, it is important to note that the $ [\\mathbf{\\bar{x}}^\\top\\mathbf{\\bar{x}}] $ must be singular positive definite (SPD) in order to obtain a unique solution . We will know our weights are trained accurate to the true weights when this equation = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d45b92",
   "metadata": {},
   "source": [
    "Question 2\n",
    "\n",
    "Now we can "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2428c",
   "metadata": {},
   "source": [
    "Question 3: Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b7ad8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_size = 8\n",
    "x1 = np.random.uniform(low=-4, high=4, size=(data_size,))\n",
    "x2 = np.random.uniform(low=-4, high=4, size=(data_size,))\n",
    "x1x2 = x1*x2\n",
    "b = np.ones(len(x1))\n",
    "X = np.array([b,x1,x2,x1x2]).T\n",
    "# w1= 3.8, w2= 1.8, w3= 0.5, b= 2.1\n",
    "W_r = np.array([2.1,3.8,1.8,0.5],dtype=np.float32)# r->real\n",
    "W = np.array([0,0,0,0])  # for estimation\n",
    "Y = np.array([x@W_r for x in X],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "11a888c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    return np.dot(X,W)\n",
    "def loss(Y,Y_pred):\n",
    "    return ((np.linalg.norm(Y_pred-Y))**2)\n",
    "def gradient(X,Y,Y_pred):\n",
    "    return X*(Y_pred-Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "596d9bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [-0.6494537   2.4171273   0.41303722  0.22920739]\n",
      "W_r = [2.1 3.8 1.8 0.5]\n"
     ]
    }
   ],
   "source": [
    "learning_rate,num_epoch = 0.001,100\n",
    "for epoch in range(num_epoch):\n",
    "    for i,x in enumerate(X):\n",
    "        y_pred  = forward(x)\n",
    "        l       = loss(y_pred,Y[i])\n",
    "        dw      = gradient(x,Y[i],y_pred)\n",
    "        W       = (W - dw*learning_rate)\n",
    "    # print(f'epoch:{epoch} | loss:{l:.8f}')\n",
    "print(f'W = {W}\\nW_r = {W_r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07447e6a",
   "metadata": {},
   "source": [
    "Question 3: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619dbc8",
   "metadata": {},
   "source": [
    "The dataset must be greater than 3 to achieve full rank. The larger the data size the more accurate. It seems that at a data set of 8 the accuracy is fairly high and the computational time is fairly low. Another issue with this solution is you must start with an initial guess of the weights. If the initial guess is far away from the correct weights it will take longer to converge on the answer or might not converge on the answer. With this solution it also contains a learning rate and an iteration limit that must be set. This could casuse the solution not to converge as well or take a longer time to converge if it is not configured correctly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97aefd60",
   "metadata": {},
   "source": [
    "Question 4: Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2c80731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 3\n",
    "\n",
    "x1 = np.random.uniform(low=-4, high=4, size=(data_size,))\n",
    "x1 = np.append(x1,x1)\n",
    "x2 = np.random.uniform(low=-4, high=4, size=(data_size,))\n",
    "x2 = np.append(x2,x2)\n",
    "x1x2 = x1*x2\n",
    "b = np.ones(len(x1))\n",
    "X = np.array([b,x1,x2,x1x2]).T\n",
    "# w1= 3.8, w2= 1.8, w3= 0.5, b= 2.1\n",
    "W_r = np.array([2.1,3.8,1.8,0.5],dtype=np.float32)# r->real\n",
    "W = np.array([0,0,0,0])  # for estimation\n",
    "Y = np.array([x@W_r for x in X],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7b432ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    return np.dot(X,W)\n",
    "def loss(Y,Y_pred):\n",
    "    return ((np.linalg.norm(Y_pred-Y))**2)\n",
    "def gradient(X,Y,Y_pred):\n",
    "    return X*(Y_pred-Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5059e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [-1.20821518  2.50798661  0.617843    0.05973311]\n",
      "W_r = [2.1 3.8 1.8 0.5]\n"
     ]
    }
   ],
   "source": [
    "learning_rate,num_epoch = 0.001,10000\n",
    "for epoch in range(num_epoch):\n",
    "    for i,x in enumerate(X):\n",
    "        y_pred  = forward(x)\n",
    "        l       = loss(y_pred,Y[i])\n",
    "        dw      = gradient(x,Y[i],y_pred)\n",
    "        W       = (W - dw*learning_rate)\n",
    "    # print(f'epoch:{epoch} | loss:{l:.8f}')\n",
    "print(f'W = {W}\\nW_r = {W_r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb846c69",
   "metadata": {},
   "source": [
    "Question 4: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af97f0",
   "metadata": {},
   "source": [
    "The rank defficiency occurs when there are less independent columns than unknowns. The addition of duplicate values further showed that the rank defficiency causes the outcome to be inaccurate. A solution to fixing this problem would come from increasing the amount of independent data points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd35435d",
   "metadata": {},
   "source": [
    "Question 5: Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b6bded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1\n",
    "x1 = np.arange(-4,4,step_size)\n",
    "x2 = np.arange(-4,4,step_size)*np.sin(np.pi)\n",
    "x1x2 = x1*x2\n",
    "b = np.ones(len(x1))\n",
    "X = np.array([b,x1,x2,x1x2]).T\n",
    "# w1= 3.8, w2= 1.8, w3= 0.5, b= 2.1\n",
    "W_r = np.array([2.1,3.8,1.8,0.5],dtype=np.float32)# r->real\n",
    "W = np.array([0,0,0,0])  # for estimation\n",
    "Y = np.array([x@W_r for x in X],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c1ce8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    return np.dot(X,W)\n",
    "def loss(Y,Y_pred):\n",
    "    return ((np.linalg.norm(Y_pred-Y))**2)\n",
    "def gradient(X,Y,Y_pred):\n",
    "    return X*(Y_pred-Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bebedf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [2.09999985e+00 3.80000002e+00 4.65365786e-16 8.14035469e-16]\n",
      "W_r = [2.1 3.8 1.8 0.5]\n"
     ]
    }
   ],
   "source": [
    "learning_rate,num_epoch = 0.001,10000\n",
    "for epoch in range(num_epoch):\n",
    "    for i,x in enumerate(X):\n",
    "        y_pred  = forward(x)\n",
    "        l       = loss(y_pred,Y[i])\n",
    "        dw      = gradient(x,Y[i],y_pred)\n",
    "        W       = (W - dw*learning_rate)\n",
    "    # print(f'epoch:{epoch} | loss:{l:.8f}')\n",
    "print(f'W = {W}\\nW_r = {W_r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb6c57",
   "metadata": {},
   "source": [
    "Question 5: Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d174342",
   "metadata": {},
   "source": [
    "The parallel data points with a sufficient amount still causes the end weights to be inaccurate. This is because there becomes more than one unique solution for the weights and bias. A way around this is possibly to rotate one of the data vectors by 90 degrees. This solution was tested and still did not produce an accurate result and is depicted above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b776a6",
   "metadata": {},
   "source": [
    "Question 6: Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "051952e7",
   "metadata": {},
   "source": [
    "Having a larger dataset such as 10,000 greatly increases the computational time with the same amount of iterations as 8 data points. The small increase in accuracy achieved by the larger dataset is not worth the increase in computational time. This means that it becomes very computational inefficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97aea390",
   "metadata": {},
   "source": [
    "Question 7: Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
